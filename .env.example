# Example environment configuration for PDF Letter Splitter
# Copy this file to .env and adjust values as needed

# =============================================================================
# LLM Server Configuration
# =============================================================================

# Path to the GGUF model file inside the container
# The model must be placed in ./models/ directory on the host
# Recommended models:
#   - Qwen2.5-7B-Instruct-Q4_K_M.gguf (default, balanced)
#   - Qwen2.5-7B-Instruct-Q5_K_M.gguf (higher quality, more VRAM)
#   - Qwen2.5-3B-Instruct-Q4_K_M.gguf (faster, lower VRAM)
LLAMA_ARG_MODEL=/models/Qwen2.5-7B-Instruct-Q4_K_M.gguf

# Context window size (tokens)
# Larger values allow processing longer documents but require more VRAM
# Default: 4096 (sufficient for letter metadata extraction)
LLAMA_ARG_CTX_SIZE=4096

# Number of GPU layers to offload (GPU profile only)
# 999 = offload all layers to GPU (recommended for GPU profile)
# 0 = CPU only (automatically set for CPU profile)
LLAMA_ARG_N_GPU_LAYERS=999

# =============================================================================
# Splitter Configuration
# =============================================================================

# Base URL of the llama-server (internal docker network address)
# Do not change this unless you're running llama-server externally
LLAMA_BASE_URL=http://llama-server:8080

# Language hint for metadata extraction
# Used in LLM prompts to indicate expected language(s)
MODEL_FIELDS_LANGUAGE_HINT=deu+eng

# Input PDF path (inside container)
# Map your host directory to /work in docker-compose
INPUT_PDF=/work/input/input.pdf

# Output directory path (inside container)
OUTPUT_DIR=/work/output
