name: pdf-letter-splitter

services:
  # LLM service using llama.cpp (CPU mode - default)
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: pdf-letter-splitter-llm-cpu
    # No host port exposure - internal only
    command: >
      --server
      --host 0.0.0.0
      --port 8080
      --model /models/${MODEL_FILE}
    volumes:
      # Mount the Source directory to access model file
      - ./:/models:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
    restart: "no"
    profiles:
      - cpu

  # LLM service with GPU support (activated with --profile gpu)
  llm-gpu:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: pdf-letter-splitter-llm-gpu
    command: >
      --server
      --host 0.0.0.0
      --port 8080
      --model /models/${MODEL_FILE}
      --n-gpu-layers 999
    volumes:
      - ./:/models:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: "no"
    profiles:
      - gpu

  # Pipeline service for OCR processing (CPU mode - default)
  pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pdf-letter-splitter-pipeline-cpu
    command:
      - "-i"
      - "${INPUT_PDF}"
      - "-o"
      - "${OUTPUT_JSON}"
      - "--jobs"
      - "${OCR_JOBS}"
    volumes:
      # Mount Source directory as /work for file access
      - ./:/work
    depends_on:
      llm:
        condition: service_healthy
    restart: "no"
    profiles:
      - cpu

  # Pipeline service for GPU profile (depends on GPU llm)
  pipeline-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pdf-letter-splitter-pipeline-gpu
    command:
      - "-i"
      - "${INPUT_PDF}"
      - "-o"
      - "${OUTPUT_JSON}"
      - "--jobs"
      - "${OCR_JOBS}"
    volumes:
      - ./:/work
    depends_on:
      llm-gpu:
        condition: service_healthy
    restart: "no"
    profiles:
      - gpu
